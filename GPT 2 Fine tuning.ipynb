# -*- coding: utf-8 -*-
"""LLM Fine tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZxfiRvkdYjaAvo068oN_7J9MiW7v-joG
"""

from datasets import load_dataset
import pandas as pd

dataset = load_dataset("mteb/tweet_sentiment_extraction")
df = pd.DataFrame(dataset['train'])

df.shape

df.head()



"""**Tokenizing **"""

from transformers import GPT2Tokenizer

# Loading the dataset to train our model
dataset = load_dataset("mteb/tweet_sentiment_extraction")

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

tokenizer.pad_token = tokenizer.eos_token
def tokenize_function(examples):
   return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

print(tokenized_datasets)

"""**Spliting Tokenized Dataset into Train and Test to evaluate our fine tuned model**"""

small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))



"""**Loading our base model**"""

from transformers import GPT2ForSequenceClassification

model = GPT2ForSequenceClassification.from_pretrained("gpt2", num_labels=3)



!pip install evaluate

import evaluate
import numpy as np

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
   logits, labels = eval_pred
   predictions = np.argmax(logits, axis=-1)
   return metric.compute(predictions=predictions, references=labels)





"""**Tuning Using the Trainer Method**"""

# Completely disable W&B
import os
os.environ["WANDB_DISABLED"] = "true"

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
   output_dir="test_trainer",
   #evaluation_strategy="epoch",
   per_device_train_batch_size=1,  # Reduce batch size here
   per_device_eval_batch_size=1,    # Optionally, reduce for evaluation as well
   gradient_accumulation_steps=4
   )


trainer = Trainer(
   model=model,
   args=training_args,
   train_dataset=small_train_dataset,
   eval_dataset=small_eval_dataset,
   compute_metrics=compute_metrics,

)

trainer.train()



"""# Evaluating the Fine Tuned Model"""

import evaluate

trainer.evaluate()



import torch
from transformers import GPT2LMHeadModel

# Make sure tokenizer uses a pad token (GPT-2 has none by default)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = tokenizer.pad_token_id

model = GPT2LMHeadModel.from_pretrained("gpt2") # Change model class

model.eval()
model.config.use_cache = True  # faster at inference

prompt = "what interview! leave me alone"
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

inputs = tokenizer(prompt, return_tensors="pt").to(device)

with torch.no_grad():
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=60,
        temperature=0.8,     # higher = more random; set to 0 for greedy
        top_p=0.95,
        do_sample=True,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.eos_token_id,
    )

print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))





"""**Testing my fine TUned Model**"""

trainer.save_model("test_trainer")         # saves model + config
tokenizer.save_pretrained("test_trainer")  # saves tokenizer

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

ckpt = "test_trainer"
tokenizer = AutoTokenizer.from_pretrained(ckpt)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(ckpt)
model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = True
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.eval()

prompt = "what interview! leave me alone"
inputs = tokenizer(prompt, return_tensors="pt").to(device)

with torch.no_grad():
    out_ids = model.generate(
        **inputs,
        max_new_tokens=60,
        temperature=0.8,
        top_p=0.95,
        do_sample=True,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.eos_token_id,
    )

print(tokenizer.decode(out_ids[0], skip_special_tokens=True))

